{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 1: Import Dependence Modules"
      ],
      "metadata": {
        "id": "1cPlGyQRjgc-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvPYt2Y6v4Dy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import copy\n",
        "import shutil\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Tuple\n",
        "from datetime import datetime\n",
        "from dataclasses import dataclass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Device"
      ],
      "metadata": {
        "id": "W5tFJykc7r0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device('cuda')\n",
        "    !nvidia-smi\n",
        "    print()\n",
        "else:\n",
        "    DEVICE = torch.device('cpu')\n",
        "print('CPU')\n",
        "!cat /proc/cpuinfo | grep 'processor\\|model\\ name'\n",
        "print()\n",
        "!python3 --version\n",
        "print('torch version   :', torch.__version__)\n",
        "print('use device      :', DEVICE)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hX-SWpcU7sm6",
        "outputId": "aabc0815-eab7-41ac-a3a2-1118dba2b7c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU\n",
            "processor\t: 0\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "processor\t: 1\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "\n",
            "Python 3.10.12\n",
            "torch version   : 2.2.1+cu121\n",
            "use device      : cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment"
      ],
      "metadata": {
        "id": "ccV4smYBxTbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TicTacToe:\n",
        "    BOARD_HEIGHT = 3\n",
        "    BOARD_WIDTH = 3\n",
        "    LINES = [[0, 1, 2], [3, 4, 5], [6, 7, 8], [0, 3, 6], [1, 4, 7], [2, 5, 8], [0, 4, 8], [2, 4, 6]]\n",
        "\n",
        "    def __init__(self):\n",
        "        self.board: np.ndarray = np.zeros(0)\n",
        "        self.action_history = []\n",
        "        self.current_player: float = 0.\n",
        "        self.winner: float = 0.\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        self.board = np.zeros(self.BOARD_HEIGHT * self.BOARD_WIDTH, dtype=np.single)\n",
        "        self.action_history.clear()\n",
        "        self.current_player = 1.\n",
        "        self.winner = 0.\n",
        "\n",
        "    def act(self, action: int) -> None:\n",
        "        if self.board[action] == 0.:\n",
        "            self.board[action] = self.current_player\n",
        "            self.action_history.append(action)\n",
        "            self._update_winner()\n",
        "            self.current_player *= -1.\n",
        "        else:\n",
        "            raise ValueError('invalid action id.')\n",
        "\n",
        "    def get_legal_actions(self) -> list:\n",
        "        return list(np.where(self.board == 0.)[0])\n",
        "\n",
        "    def is_terminal(self) -> bool:\n",
        "        return self.winner != 0. or not np.any(self.board == 0.)\n",
        "\n",
        "    def get_eval_score(self) -> float:\n",
        "        return self.winner\n",
        "\n",
        "    def get_features(self) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        4 channels\n",
        "            0. own position\n",
        "            1. opponent position\n",
        "            2. Nought turn\n",
        "            3. Cross turn\n",
        "        \"\"\"\n",
        "        features = []\n",
        "        for channel in range(4):\n",
        "            if channel == 0:\n",
        "                features.append(np.where(self.board == self.current_player, 1., 0.))\n",
        "            elif channel == 1:\n",
        "                features.append(np.where(self.board == self.current_player * -1., 1., 0.))\n",
        "            elif channel == 2:\n",
        "                features.append(np.ones_like(self.board) if self.current_player == 1. else np.zeros_like(self.board))\n",
        "            elif channel == 3:\n",
        "                features.append(np.ones_like(self.board) if self.current_player == -1. else np.zeros_like(self.board))\n",
        "        return np.stack(features, dtype=np.single).reshape((-1, self.BOARD_HEIGHT, self.BOARD_WIDTH))\n",
        "\n",
        "    @staticmethod\n",
        "    def get_num_input_channels() -> int:\n",
        "        return 4\n",
        "\n",
        "    def get_input_channel_height(self) -> int:\n",
        "        return self.BOARD_HEIGHT\n",
        "\n",
        "    def get_input_channel_width(self) -> int:\n",
        "        return self.BOARD_WIDTH\n",
        "\n",
        "    def get_policy_size(self) -> int:\n",
        "        return self.BOARD_HEIGHT * self.BOARD_WIDTH\n",
        "\n",
        "    def to_string(self) -> str:\n",
        "        result = np.empty(self.board.shape, dtype=str)\n",
        "        result[np.where(self.board == 0.)] = ' '\n",
        "        result[np.where(self.board == 1.)] = 'O'\n",
        "        result[np.where(self.board == -1.)] = 'X'\n",
        "        return str(result.reshape((self.BOARD_HEIGHT, self.BOARD_WIDTH)))\n",
        "\n",
        "    def _update_winner(self) -> None:\n",
        "        for line in self.LINES:\n",
        "            line_values = self.board[line]\n",
        "            if np.all(line_values != 0.) and np.all(line_values == self.current_player):\n",
        "                self.winner = self.current_player\n"
      ],
      "metadata": {
        "id": "kgHek7TPwTnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConnectFour(TicTacToe):\n",
        "    BOARD_HEIGHT = 6\n",
        "    BOARD_WIDTH = 7\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.board_view: np.ndarray = np.zeros(0)\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        super().reset()\n",
        "        self.board_view = self.board.reshape(ConnectFour.BOARD_HEIGHT, ConnectFour.BOARD_WIDTH)\n",
        "\n",
        "    def act(self, action: int) -> None:\n",
        "        action_row = np.argmax(self.board_view == 0., axis=0)[action]\n",
        "        if self.board_view[action_row, action] == 0.:\n",
        "            self.board[action_row * self.BOARD_WIDTH + action] = self.current_player\n",
        "            self.board_view[action_row, action] = self.current_player\n",
        "            self.action_history.append(action)\n",
        "            num_connection = self._check_connected(action_row, action)\n",
        "            if num_connection >= 4:\n",
        "                self.winner = self.current_player\n",
        "            self.current_player *= -1.\n",
        "        else:\n",
        "            raise ValueError('invalid action id.')\n",
        "\n",
        "    def get_legal_actions(self) -> list:\n",
        "        non_zero_mask = np.any(self.board_view == 0., axis=0)\n",
        "        return list(np.where(non_zero_mask)[0])\n",
        "\n",
        "    def get_policy_size(self) -> int:\n",
        "        return self.BOARD_WIDTH\n",
        "\n",
        "    def to_string(self) -> str:\n",
        "        result = np.empty(self.board_view.shape, dtype=str)\n",
        "        result[np.where(self.board_view == 1.)] = 'O'\n",
        "        result[np.where(self.board_view == -1.)] = 'X'\n",
        "        result[np.where(self.board_view == 0.)] = ' '\n",
        "        return str(np.flip(result, 0))\n",
        "\n",
        "    def _check_connected(self, action_row: int, action_column: int) -> int:\n",
        "        row_array = self.board_view[action_row, :]\n",
        "        column_array = self.board_view[:, action_column]\n",
        "        diagonal_array = self.board_view.diagonal(offset=action_column - action_row)\n",
        "        flipped = np.fliplr(self.board_view)\n",
        "        flipped_diagonal_array = flipped.diagonal(offset=self.BOARD_WIDTH - 1 - action_column - action_row)\n",
        "        max_connections = []\n",
        "        action_player = self.board_view[action_row, action_column]\n",
        "        for arr in [row_array, column_array, diagonal_array, flipped_diagonal_array]:\n",
        "            split_index = np.where(arr != action_player)[0]\n",
        "            split_index = np.insert(split_index, 0, -1)\n",
        "            split_index = np.insert(split_index, len(split_index), len(arr))\n",
        "            max_connections.append(np.max(np.diff(split_index)))\n",
        "        return max(max_connections) - 1\n"
      ],
      "metadata": {
        "id": "dpmBmnEZkhxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test Environment"
      ],
      "metadata": {
        "id": "199wnPfxxofm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seeds = [100, 200, 300]\n",
        "for seed in seeds:\n",
        "    np.random.seed(seed)\n",
        "    env = TicTacToe()\n",
        "    env.reset()\n",
        "    while not env.is_terminal():\n",
        "        legal_actions = env.get_legal_actions()\n",
        "        action_id = np.random.choice(legal_actions)\n",
        "        print('player:', env.current_player, '(O)' if env.current_player == 1.0 else '(X)')\n",
        "        print('board:')\n",
        "        print(env.to_string())\n",
        "        print('features:')\n",
        "        print(env.get_features())\n",
        "        env.act(action_id)\n",
        "        print('-----------------------')\n",
        "        print('act action id:', action_id)\n",
        "\n",
        "    print('score:', env.get_eval_score())\n",
        "    print(env.to_string())\n",
        "    print('========================')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO4HLm_hxzs1",
        "outputId": "fff19758-b497-4bd8-8a9c-b8f72e2e64d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "player: 1.0 (O)\n",
            "board:\n",
            "[[' ' ' ' ' ']\n",
            " [' ' ' ' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "features:\n",
            "[[[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n",
            "-----------------------\n",
            "act action id: 8\n",
            "player: -1.0 (X)\n",
            "board:\n",
            "[[' ' ' ' ' ']\n",
            " [' ' ' ' ' ']\n",
            " [' ' ' ' 'O']]\n",
            "features:\n",
            "[[[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 1.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]]]\n",
            "-----------------------\n",
            "act action id: 0\n",
            "player: 1.0 (O)\n",
            "board:\n",
            "[['X' ' ' ' ']\n",
            " [' ' ' ' ' ']\n",
            " [' ' ' ' 'O']]\n",
            "features:\n",
            "[[[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 1.]]\n",
            "\n",
            " [[1. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n",
            "-----------------------\n",
            "act action id: 4\n",
            "player: -1.0 (X)\n",
            "board:\n",
            "[['X' ' ' ' ']\n",
            " [' ' 'O' ' ']\n",
            " [' ' ' ' 'O']]\n",
            "features:\n",
            "[[[1. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 1. 0.]\n",
            "  [0. 0. 1.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]]]\n",
            "-----------------------\n",
            "act action id: 1\n",
            "player: 1.0 (O)\n",
            "board:\n",
            "[['X' 'X' ' ']\n",
            " [' ' 'O' ' ']\n",
            " [' ' ' ' 'O']]\n",
            "features:\n",
            "[[[0. 0. 0.]\n",
            "  [0. 1. 0.]\n",
            "  [0. 0. 1.]]\n",
            "\n",
            " [[1. 1. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n",
            "-----------------------\n",
            "act action id: 5\n",
            "player: -1.0 (X)\n",
            "board:\n",
            "[['X' 'X' ' ']\n",
            " [' ' 'O' 'O']\n",
            " [' ' ' ' 'O']]\n",
            "features:\n",
            "[[[1. 1. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 1. 1.]\n",
            "  [0. 0. 1.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]]]\n",
            "-----------------------\n",
            "act action id: 6\n",
            "player: 1.0 (O)\n",
            "board:\n",
            "[['X' 'X' ' ']\n",
            " [' ' 'O' 'O']\n",
            " ['X' ' ' 'O']]\n",
            "features:\n",
            "[[[0. 0. 0.]\n",
            "  [0. 1. 1.]\n",
            "  [0. 0. 1.]]\n",
            "\n",
            " [[1. 1. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [1. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n",
            "-----------------------\n",
            "act action id: 2\n",
            "score: 1.0\n",
            "[['X' 'X' 'O']\n",
            " [' ' 'O' 'O']\n",
            " ['X' ' ' 'O']]\n",
            "========================\n",
            "player: 1.0 (O)\n",
            "board:\n",
            "[[' ' ' ' ' ']\n",
            " [' ' ' ' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "features:\n",
            "[[[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n",
            "-----------------------\n",
            "act action id: 0\n",
            "player: -1.0 (X)\n",
            "board:\n",
            "[['O' ' ' ' ']\n",
            " [' ' ' ' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "features:\n",
            "[[[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]]]\n",
            "-----------------------\n",
            "act action id: 5\n",
            "player: 1.0 (O)\n",
            "board:\n",
            "[['O' ' ' ' ']\n",
            " [' ' ' ' 'X']\n",
            " [' ' ' ' ' ']]\n",
            "features:\n",
            "[[[1. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 1.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n",
            "-----------------------\n",
            "act action id: 3\n",
            "player: -1.0 (X)\n",
            "board:\n",
            "[['O' ' ' ' ']\n",
            " ['O' ' ' 'X']\n",
            " [' ' ' ' ' ']]\n",
            "features:\n",
            "[[[0. 0. 0.]\n",
            "  [0. 0. 1.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0.]\n",
            "  [1. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]]]\n",
            "-----------------------\n",
            "act action id: 7\n",
            "player: 1.0 (O)\n",
            "board:\n",
            "[['O' ' ' ' ']\n",
            " ['O' ' ' 'X']\n",
            " [' ' 'X' ' ']]\n",
            "features:\n",
            "[[[1. 0. 0.]\n",
            "  [1. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 1.]\n",
            "  [0. 1. 0.]]\n",
            "\n",
            " [[1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n",
            "-----------------------\n",
            "act action id: 2\n",
            "player: -1.0 (X)\n",
            "board:\n",
            "[['O' ' ' 'O']\n",
            " ['O' ' ' 'X']\n",
            " [' ' 'X' ' ']]\n",
            "features:\n",
            "[[[0. 0. 0.]\n",
            "  [0. 0. 1.]\n",
            "  [0. 1. 0.]]\n",
            "\n",
            " [[1. 0. 1.]\n",
            "  [1. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]]]\n",
            "-----------------------\n",
            "act action id: 6\n",
            "player: 1.0 (O)\n",
            "board:\n",
            "[['O' ' ' 'O']\n",
            " ['O' ' ' 'X']\n",
            " ['X' 'X' ' ']]\n",
            "features:\n",
            "[[[1. 0. 1.]\n",
            "  [1. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 1.]\n",
            "  [1. 1. 0.]]\n",
            "\n",
            " [[1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n",
            "-----------------------\n",
            "act action id: 4\n",
            "player: -1.0 (X)\n",
            "board:\n",
            "[['O' ' ' 'O']\n",
            " ['O' 'O' 'X']\n",
            " ['X' 'X' ' ']]\n",
            "features:\n",
            "[[[0. 0. 0.]\n",
            "  [0. 0. 1.]\n",
            "  [1. 1. 0.]]\n",
            "\n",
            " [[1. 0. 1.]\n",
            "  [1. 1. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]]]\n",
            "-----------------------\n",
            "act action id: 8\n",
            "score: -1.0\n",
            "[['O' ' ' 'O']\n",
            " ['O' 'O' 'X']\n",
            " ['X' 'X' 'X']]\n",
            "========================\n",
            "player: 1.0 (O)\n",
            "board:\n",
            "[[' ' ' ' ' ']\n",
            " [' ' ' ' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "features:\n",
            "[[[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n",
            "-----------------------\n",
            "act action id: 1\n",
            "player: -1.0 (X)\n",
            "board:\n",
            "[[' ' 'O' ' ']\n",
            " [' ' ' ' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "features:\n",
            "[[[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]]]\n",
            "-----------------------\n",
            "act action id: 2\n",
            "player: 1.0 (O)\n",
            "board:\n",
            "[[' ' 'O' 'X']\n",
            " [' ' ' ' ' ']\n",
            " [' ' ' ' ' ']]\n",
            "features:\n",
            "[[[0. 1. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 0. 1.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n",
            "-----------------------\n",
            "act action id: 7\n",
            "player: -1.0 (X)\n",
            "board:\n",
            "[[' ' 'O' 'X']\n",
            " [' ' ' ' ' ']\n",
            " [' ' 'O' ' ']]\n",
            "features:\n",
            "[[[0. 0. 1.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 1. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]]]\n",
            "-----------------------\n",
            "act action id: 4\n",
            "player: 1.0 (O)\n",
            "board:\n",
            "[[' ' 'O' 'X']\n",
            " [' ' 'X' ' ']\n",
            " [' ' 'O' ' ']]\n",
            "features:\n",
            "[[[0. 1. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 1. 0.]]\n",
            "\n",
            " [[0. 0. 1.]\n",
            "  [0. 1. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n",
            "-----------------------\n",
            "act action id: 5\n",
            "player: -1.0 (X)\n",
            "board:\n",
            "[[' ' 'O' 'X']\n",
            " [' ' 'X' 'O']\n",
            " [' ' 'O' ' ']]\n",
            "features:\n",
            "[[[0. 0. 1.]\n",
            "  [0. 1. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0.]\n",
            "  [0. 0. 1.]\n",
            "  [0. 1. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]]]\n",
            "-----------------------\n",
            "act action id: 3\n",
            "player: 1.0 (O)\n",
            "board:\n",
            "[[' ' 'O' 'X']\n",
            " ['X' 'X' 'O']\n",
            " [' ' 'O' ' ']]\n",
            "features:\n",
            "[[[0. 1. 0.]\n",
            "  [0. 0. 1.]\n",
            "  [0. 1. 0.]]\n",
            "\n",
            " [[0. 0. 1.]\n",
            "  [1. 1. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n",
            "-----------------------\n",
            "act action id: 6\n",
            "player: -1.0 (X)\n",
            "board:\n",
            "[[' ' 'O' 'X']\n",
            " ['X' 'X' 'O']\n",
            " ['O' 'O' ' ']]\n",
            "features:\n",
            "[[[0. 0. 1.]\n",
            "  [1. 1. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[0. 1. 0.]\n",
            "  [0. 0. 1.]\n",
            "  [1. 1. 0.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]\n",
            "\n",
            " [[1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]]]\n",
            "-----------------------\n",
            "act action id: 8\n",
            "player: 1.0 (O)\n",
            "board:\n",
            "[[' ' 'O' 'X']\n",
            " ['X' 'X' 'O']\n",
            " ['O' 'O' 'X']]\n",
            "features:\n",
            "[[[0. 1. 0.]\n",
            "  [0. 0. 1.]\n",
            "  [1. 1. 0.]]\n",
            "\n",
            " [[0. 0. 1.]\n",
            "  [1. 1. 0.]\n",
            "  [0. 0. 1.]]\n",
            "\n",
            " [[1. 1. 1.]\n",
            "  [1. 1. 1.]\n",
            "  [1. 1. 1.]]\n",
            "\n",
            " [[0. 0. 0.]\n",
            "  [0. 0. 0.]\n",
            "  [0. 0. 0.]]]\n",
            "-----------------------\n",
            "act action id: 0\n",
            "score: 0.0\n",
            "[['O' 'O' 'X']\n",
            " ['X' 'X' 'O']\n",
            " ['O' 'O' 'X']]\n",
            "========================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MCTS"
      ],
      "metadata": {
        "id": "qXQEhaARmKju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    def __init__(self, player: float, action: int, policy: float = 0):\n",
        "        self.player: float = player\n",
        "        self.action: int = action\n",
        "        self.policy: float = policy\n",
        "        self.visit_count: int = 0\n",
        "        self.mean_value: float = 0.\n",
        "        self.children: list[Node] = []\n",
        "\n",
        "    def is_leaf(self) -> bool:\n",
        "        return len(self.children) == 0\n",
        "\n",
        "    def add_child(self, player: float, action: int, policy: float) -> None:\n",
        "        child_node = Node(player, action, policy)\n",
        "        self.children.append(child_node)\n",
        "\n",
        "    def update(self, value: float) -> None:\n",
        "        self.visit_count += 1\n",
        "        self.mean_value += (value - self.mean_value) / self.visit_count\n",
        "\n",
        "\n",
        "class MCTS:\n",
        "    DIRICHLET_NOISE_ALPHA = 0.3  # Usually (1 / sqrt(number of actions))\n",
        "    DIRICHLET_NOISE_EPSILON = 0.25\n",
        "    PUCT_C1 = 1.25\n",
        "    PUCT_C2 = 19652\n",
        "    TEMPERATURE = 1.0\n",
        "\n",
        "    def __init__(self, env: TicTacToe, network: nn.Module):\n",
        "        self.env: TicTacToe = env\n",
        "        self.network: nn.Module = network\n",
        "        self.root: Node | None = None\n",
        "\n",
        "    def simulate(self, num_simulations: int, use_dirichlet_noise: bool = True) -> None:\n",
        "        self.root = Node(self.env.current_player, 0)\n",
        "        node_path = [self.root]\n",
        "        self.expand_and_evaluate(node_path)\n",
        "        if use_dirichlet_noise:\n",
        "            self.add_exploration_noise(self.root)\n",
        "\n",
        "        for _ in range(num_simulations):\n",
        "            # Selection\n",
        "            node_path = self.selection()\n",
        "            # Expansion and evaluation\n",
        "            value = self.expand_and_evaluate(node_path)\n",
        "            # Backup\n",
        "            self.backup(node_path, value)\n",
        "\n",
        "    def selection(self) -> list[Node]:\n",
        "        node_path = [self.root]\n",
        "        node = self.root\n",
        "        while not node.is_leaf():\n",
        "            node = self.select_child(node)\n",
        "            node_path.append(node)\n",
        "        return node_path\n",
        "\n",
        "    def expand_and_evaluate(self, node_path) -> float:\n",
        "        # go to state\n",
        "        env_transition = copy.deepcopy(self.env)\n",
        "        for child in node_path[1:]:\n",
        "            env_transition.act(child.action)\n",
        "\n",
        "        if env_transition.is_terminal():\n",
        "            return env_transition.get_eval_score()\n",
        "\n",
        "        features = env_transition.get_features()\n",
        "        features_tensor = torch.from_numpy(features).unsqueeze(0).to(DEVICE)\n",
        "        _, policy, value = self.network(features_tensor)\n",
        "        policy = policy.squeeze(0)\n",
        "        player = env_transition.current_player\n",
        "        leaf_node = node_path[-1]\n",
        "        for action in env_transition.get_legal_actions():\n",
        "            leaf_node.add_child(player, action, policy[action].item())\n",
        "        return value.squeeze(0).item()\n",
        "\n",
        "    @staticmethod\n",
        "    def backup(node_path: list[Node], value: float) -> None:\n",
        "        for node in node_path:\n",
        "            node.update(value * node.player)\n",
        "\n",
        "    def decide_action(self, use_softmax: bool = True) -> int:\n",
        "        candidate_actions = []\n",
        "        action_weights = []\n",
        "        for child in self.root.children:\n",
        "            candidate_actions.append(child.action)\n",
        "            action_weights.append(child.visit_count ** (1 / self.TEMPERATURE))\n",
        "        if np.sum(action_weights) == 0:\n",
        "            action_weights = np.ones_like(action_weights, dtype=np.single)\n",
        "        action_weights /= np.sum(action_weights)\n",
        "        if use_softmax:\n",
        "            selected_action = np.random.choice(candidate_actions, p=action_weights)\n",
        "        else:\n",
        "            selected_action = candidate_actions[np.argmax(action_weights)]\n",
        "        return selected_action\n",
        "\n",
        "    def add_exploration_noise(self, parent: Node) -> None:\n",
        "        dirichlet_noise = np.random.gamma(self.DIRICHLET_NOISE_ALPHA, 1, len(parent.children))\n",
        "        dirichlet_noise /= np.sum(dirichlet_noise)\n",
        "        for child, noise in zip(parent.children, dirichlet_noise):\n",
        "            child.policy = child.policy * (1 - self.DIRICHLET_NOISE_EPSILON) + noise * self.DIRICHLET_NOISE_EPSILON\n",
        "\n",
        "    def select_child(self, parent: Node) -> Node:\n",
        "        # ============== TODO ==============\n",
        "        # hint: select the child with the highest PUCT score\n",
        "        # hint: self.PUCT_C1 and self.PUCT_C2 is PUCT constant\n",
        "        best_child = np.random.choice(parent.children)\n",
        "        return best_child\n",
        "\n",
        "    def get_normalize_child_visits(self) -> np.ndarray:\n",
        "        child_visits = np.zeros(self.env.get_policy_size(), dtype=np.single)\n",
        "        for child in self.root.children:\n",
        "            child_visits[child.action] = child.visit_count / self.root.visit_count\n",
        "        return child_visits\n"
      ],
      "metadata": {
        "id": "kTzL4EgfmLO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replay Buffer"
      ],
      "metadata": {
        "id": "qdanapkXnzep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Game:\n",
        "    action_history: list[int]\n",
        "    terminal_value: float\n",
        "    child_visits: list[np.ndarray]\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_num_games: int):\n",
        "        self.buffer: list[Game] = []\n",
        "        self.buffer_num_games: int = buffer_num_games\n",
        "\n",
        "    def save_game(self, game: Game) -> None:\n",
        "        if len(self.buffer) >= self.buffer_num_games:\n",
        "            self.buffer.pop(0)\n",
        "        self.buffer.append(game)\n",
        "\n",
        "    def sample_batch(self, env: TicTacToe, batch_size: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "        features = []\n",
        "        target_value = []\n",
        "        target_policy = []\n",
        "        select_weights = np.array([len(game.action_history) for game in self.buffer])\n",
        "        select_weights = select_weights / np.sum(select_weights)\n",
        "        sample_games_index = np.random.choice(len(self.buffer), size=batch_size, p=select_weights, replace=True)\n",
        "        unique, counts = np.unique(sample_games_index, return_counts=True)\n",
        "        for game_index, count in zip(unique, counts):\n",
        "            env.reset()\n",
        "            game = self.buffer[game_index]\n",
        "            env_steps = np.random.randint(len(game.action_history), size=count)\n",
        "            env_steps.sort()\n",
        "            for step in env_steps:\n",
        "                for action in game.action_history[len(env.action_history): step]:\n",
        "                    env.act(action)\n",
        "                features.append(env.get_features())\n",
        "                target_value.append(game.terminal_value)\n",
        "                target_policy.append(game.child_visits[step])\n",
        "\n",
        "        features = np.stack(features, dtype=np.single)\n",
        "        target_policy = np.stack(target_policy, dtype=np.single)\n",
        "        target_value = np.stack(target_value, dtype=np.single)\n",
        "        return features, target_policy, target_value\n"
      ],
      "metadata": {
        "id": "OiJtymUHnzw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AlphaZero Network"
      ],
      "metadata": {
        "id": "6oe6TjYZodpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AlphaZeroNetwork(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_channels: int,\n",
        "                 input_channel_height: int,\n",
        "                 input_channel_width: int,\n",
        "                 action_size: int,\n",
        "                 hidden_channels: int = 16,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(input_channels, hidden_channels, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(hidden_channels)\n",
        "        self.conv2 = nn.Conv2d(hidden_channels, 1, kernel_size=1)\n",
        "        self.bn2 = nn.BatchNorm2d(1)\n",
        "        self.flat = nn.Flatten()\n",
        "        self.policy_head = nn.Linear(input_channel_height * input_channel_width, action_size)\n",
        "        self.value_head = nn.Linear(input_channel_height * input_channel_width, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.functional.relu(self.bn1(self.conv1(x)))\n",
        "        x = nn.functional.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.flat(x)\n",
        "        p = self.policy_head(x)\n",
        "        policy_logit = nn.functional.log_softmax(p, dim=1)\n",
        "        policy = nn.functional.softmax(p, dim=1)\n",
        "        value = torch.tanh(self.value_head(x))\n",
        "        return policy_logit, policy, value\n"
      ],
      "metadata": {
        "id": "-FwMy49-od0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logger"
      ],
      "metadata": {
        "id": "ag14ObvZo6H0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def now_time() -> str:\n",
        "    return datetime.now().strftime(\"[%Y-%m-%d %H:%M:%S] \")\n",
        "\n",
        "\n",
        "class Logger:\n",
        "    def __init__(self, env: TicTacToe, device: torch.device | str = 'cpu'):\n",
        "        self.folder_name: str = f'{type(env).__name__}'\n",
        "        self.model_folder_name: str = f'{type(env).__name__}/model'\n",
        "        self.device = device\n",
        "        if os.path.exists(self.model_folder_name):\n",
        "            shutil.rmtree(self.model_folder_name)\n",
        "        os.makedirs(self.model_folder_name)\n",
        "\n",
        "\n",
        "    def write_log(self, write_message: str, timestamp: bool = True) -> None:\n",
        "        with open(f'{self.folder_name}/training_log.txt', 'a') as f:\n",
        "            if timestamp:\n",
        "                write_message = now_time() + write_message\n",
        "            f.write(write_message + '\\n')\n",
        "            print(write_message)\n",
        "\n",
        "    def save_network(self, network: nn.Module, iteration: int) -> None:\n",
        "        torch.jit.script(network).save(f'{self.model_folder_name}/weight_iter_{iteration}.pt')\n",
        "\n",
        "    def load_network(self, iteration: int) -> nn.Module:\n",
        "        return torch.jit.load(f'{self.model_folder_name}/weight_iter_{iteration}.pt',\n",
        "                              map_location=torch.device(self.device))\n"
      ],
      "metadata": {
        "id": "oRziFBR8o6aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 2: AlphaZero Algorithm"
      ],
      "metadata": {
        "id": "s2gjd3aQpgX4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-Play"
      ],
      "metadata": {
        "id": "6MCtN9QUqLNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def self_play(env: TicTacToe,\n",
        "              network: nn.Module,\n",
        "              logger: Logger,\n",
        "              sp_num_games_per_iteration: int,\n",
        "              sp_mcts_simulation: int,\n",
        "              display_step: int = 10) -> list[Game]:\n",
        "    network.eval()\n",
        "    games = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(1, sp_num_games_per_iteration + 1):\n",
        "            search_statistics = []\n",
        "            env.reset()\n",
        "            while not env.is_terminal():\n",
        "                mcts = MCTS(copy.deepcopy(env), network)\n",
        "                mcts.simulate(sp_mcts_simulation)\n",
        "                action = mcts.decide_action()\n",
        "                env.act(action)\n",
        "                search_statistics.append(mcts.get_normalize_child_visits())\n",
        "\n",
        "            assert len(env.action_history) == len(search_statistics)\n",
        "            game = Game(env.action_history[:], env.get_eval_score(), search_statistics)\n",
        "            games.append(game)\n",
        "\n",
        "            if i % display_step == 0:\n",
        "                logger.write_log(f'sp games: {i} / {sp_num_games_per_iteration}')\n",
        "    return games\n"
      ],
      "metadata": {
        "id": "1gI1SIooqJuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimization"
      ],
      "metadata": {
        "id": "symzwpM0qlnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimization(env: TicTacToe,\n",
        "                 network: nn.Module,\n",
        "                 logger: Logger,\n",
        "                 replay_buffer: ReplayBuffer,\n",
        "                 op_batch_size: int,\n",
        "                 op_training_steps: int,\n",
        "                 display_step: int = 10,\n",
        "                 learning_rate: float = 0.02,\n",
        "                 momentum: float = 0.9,\n",
        "                 weight_decay: float = 1e-4) -> None:\n",
        "    network.train()\n",
        "    optimizer = torch.optim.SGD(network.parameters(),\n",
        "                                lr=learning_rate,\n",
        "                                momentum=momentum,\n",
        "                                weight_decay=weight_decay)\n",
        "    for i in range(1, op_training_steps + 1):\n",
        "        optimizer.zero_grad()\n",
        "        features, target_policy, target_value = replay_buffer.sample_batch(env, op_batch_size)\n",
        "        features_tensor = torch.from_numpy(features).to(DEVICE)\n",
        "        target_policy_tensor = torch.from_numpy(target_policy).to(DEVICE)\n",
        "        target_value_tensor = torch.from_numpy(target_value).unsqueeze(-1).to(DEVICE)\n",
        "\n",
        "        policy_logit, _, value = network(features_tensor)\n",
        "        loss_policy = nn.functional.cross_entropy(policy_logit, target_policy_tensor)\n",
        "        loss_value = nn.functional.mse_loss(value, target_value_tensor)\n",
        "        loss = loss_policy + loss_value\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % display_step == 0:\n",
        "            logger.write_log(f'op training steps: {i} / {op_training_steps}')\n",
        "            logger.write_log(f'\\tloss_policy: {loss_policy.item():.4f}', timestamp=False)\n",
        "            logger.write_log(f'\\tloss_value : {loss_value.item():.4f}', timestamp=False)\n"
      ],
      "metadata": {
        "id": "hD7SK1adqlvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "Ez5WgT3QqvFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation(env: TicTacToe,\n",
        "               logger: Logger,\n",
        "               iteration_a: int,\n",
        "               iteration_b: int,\n",
        "               num_games: int,\n",
        "               mcts_simulation: int):\n",
        "    network_a = logger.load_network(iteration_a)\n",
        "    network_b = logger.load_network(iteration_b)\n",
        "    network_a.eval()\n",
        "    network_b.eval()\n",
        "    winner_count = [{'iter': iteration_a, 'winP1': 0, 'winP2': 0, 'draw': 0, 'lossP1': 0, 'lossP2': 0, 'scores': []},\n",
        "                    {'iter': iteration_b, 'winP1': 0, 'winP2': 0, 'draw': 0, 'lossP1': 0, 'lossP2': 0, 'scores': []}]\n",
        "    with torch.no_grad():\n",
        "        for i in range(num_games):\n",
        "            env.reset()\n",
        "            while not env.is_terminal():\n",
        "                network = network_a if env.current_player == 1. else network_b\n",
        "                mcts = MCTS(copy.deepcopy(env), network)\n",
        "                mcts.simulate(mcts_simulation, use_dirichlet_noise=False)\n",
        "                action = mcts.decide_action(use_softmax=True)\n",
        "                env.act(action)\n",
        "            eval_score = env.get_eval_score()\n",
        "            winner_count[0]['scores'].append(eval_score)\n",
        "            winner_count[1]['scores'].append(-eval_score)\n",
        "            if eval_score == 1.:\n",
        "                winner_count[0]['winP1'] += 1\n",
        "                winner_count[1]['lossP2'] += 1\n",
        "            elif eval_score == -1.:\n",
        "                winner_count[0]['lossP1'] += 1\n",
        "                winner_count[1]['winP2'] += 1\n",
        "            else:\n",
        "                winner_count[0]['draw'] += 1\n",
        "                winner_count[1]['draw'] += 1\n",
        "            # Swap player\n",
        "            network_a, network_b = network_b, network_a\n",
        "            winner_count[0], winner_count[1] = winner_count[1], winner_count[0]\n",
        "\n",
        "    sorted(winner_count, key=lambda item: item['iter'])\n",
        "    own_info = winner_count.pop()\n",
        "    opponent_info = winner_count.pop()\n",
        "    title_name = f'iteration {own_info.pop(\"iter\")} vs. {opponent_info.pop(\"iter\")}'\n",
        "    mean_score = sum(own_info.pop('scores')) / num_games\n",
        "    logger.write_log(f'\\t{title_name}: {own_info}', timestamp=False)\n",
        "    logger.write_log(f'\\t{title_name} win rate: {(mean_score + 1) / 2:.2%}', timestamp=False)\n"
      ],
      "metadata": {
        "id": "9qnKzdB7qPxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Model (Main Function)"
      ],
      "metadata": {
        "id": "s0Op9M2_sH_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "max_iteration = 50\n",
        "sp_num_games_per_iteration = 10\n",
        "sp_mcts_simulation = 8\n",
        "buffer_num_iteration = 5\n",
        "op_batch_size = 32\n",
        "op_training_steps = 50\n",
        "eval_interval_iteration = 5\n",
        "eval_num_games = 100\n",
        "network_num_hidden_channels = 8\n",
        "\n",
        "random_seed = 600\n",
        "np.random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "env = TicTacToe()\n",
        "# env = ConnectFour()\n",
        "network = AlphaZeroNetwork(input_channels=env.get_num_input_channels(),\n",
        "                           input_channel_height=env.get_input_channel_height(),\n",
        "                           input_channel_width=env.get_input_channel_width(),\n",
        "                           action_size=env.get_policy_size(),\n",
        "                           hidden_channels=network_num_hidden_channels\n",
        "                           ).to(DEVICE)\n",
        "replay_buffer = ReplayBuffer(buffer_num_iteration * sp_num_games_per_iteration)\n",
        "logger = Logger(env, device=DEVICE)\n",
        "logger.save_network(network, 0)\n",
        "\n",
        "for i in range(1, max_iteration + 1):\n",
        "    logger.write_log(f'iteration {i}:')\n",
        "\n",
        "    # Part 1: Self-Play\n",
        "    latest_network = logger.load_network(i - 1)\n",
        "    games = self_play(env, latest_network, logger,\n",
        "                      sp_num_games_per_iteration=sp_num_games_per_iteration,\n",
        "                      sp_mcts_simulation=sp_mcts_simulation,\n",
        "                      display_step=10)\n",
        "    for game in games:\n",
        "        replay_buffer.save_game(game)\n",
        "\n",
        "    # Part 2: Training\n",
        "    optimization(env, network, logger, replay_buffer,\n",
        "                 op_batch_size=op_batch_size,\n",
        "                 op_training_steps=op_training_steps,\n",
        "                 display_step=10)\n",
        "    logger.save_network(network, i)\n",
        "\n",
        "    # Evaluation\n",
        "    if i % eval_interval_iteration == 0:\n",
        "        logger.write_log('evaluation:')\n",
        "        evaluation(env, logger,\n",
        "                   iteration_a=0,\n",
        "                   iteration_b=i,\n",
        "                   num_games=eval_num_games,\n",
        "                   mcts_simulation=sp_mcts_simulation)\n",
        "        if i - eval_interval_iteration > 0:\n",
        "            evaluation(env, logger,\n",
        "                    iteration_a=i - eval_interval_iteration,\n",
        "                    iteration_b=i,\n",
        "                    num_games=eval_num_games,\n",
        "                    mcts_simulation=sp_mcts_simulation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBFU2_wpsXov",
        "outputId": "3209a547-ce9f-4a37-c1fb-7ae62b0590a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-04-18 10:29:08] iteration 1:\n",
            "[2024-04-18 10:29:09] sp games: 10 / 10\n",
            "[2024-04-18 10:29:09] op training steps: 10 / 50\n",
            "\tloss_policy: 2.1167\n",
            "\tloss_value : 0.5453\n",
            "[2024-04-18 10:29:09] op training steps: 20 / 50\n",
            "\tloss_policy: 2.1662\n",
            "\tloss_value : 0.5785\n",
            "[2024-04-18 10:29:09] op training steps: 30 / 50\n",
            "\tloss_policy: 2.0788\n",
            "\tloss_value : 0.3294\n",
            "[2024-04-18 10:29:09] op training steps: 40 / 50\n",
            "\tloss_policy: 2.1420\n",
            "\tloss_value : 0.4288\n",
            "[2024-04-18 10:29:09] op training steps: 50 / 50\n",
            "\tloss_policy: 2.0470\n",
            "\tloss_value : 0.2275\n",
            "[2024-04-18 10:29:09] iteration 2:\n",
            "[2024-04-18 10:29:10] sp games: 10 / 10\n",
            "[2024-04-18 10:29:10] op training steps: 10 / 50\n",
            "\tloss_policy: 2.0597\n",
            "\tloss_value : 0.5637\n",
            "[2024-04-18 10:29:10] op training steps: 20 / 50\n",
            "\tloss_policy: 2.0099\n",
            "\tloss_value : 0.4261\n",
            "[2024-04-18 10:29:10] op training steps: 30 / 50\n",
            "\tloss_policy: 1.9436\n",
            "\tloss_value : 0.5477\n",
            "[2024-04-18 10:29:10] op training steps: 40 / 50\n",
            "\tloss_policy: 1.9575\n",
            "\tloss_value : 0.4956\n",
            "[2024-04-18 10:29:10] op training steps: 50 / 50\n",
            "\tloss_policy: 1.8903\n",
            "\tloss_value : 0.4081\n",
            "[2024-04-18 10:29:10] iteration 3:\n",
            "[2024-04-18 10:29:11] sp games: 10 / 10\n",
            "[2024-04-18 10:29:11] op training steps: 10 / 50\n",
            "\tloss_policy: 1.9768\n",
            "\tloss_value : 0.8284\n",
            "[2024-04-18 10:29:11] op training steps: 20 / 50\n",
            "\tloss_policy: 1.7887\n",
            "\tloss_value : 0.7063\n",
            "[2024-04-18 10:29:11] op training steps: 30 / 50\n",
            "\tloss_policy: 1.5228\n",
            "\tloss_value : 0.6163\n",
            "[2024-04-18 10:29:11] op training steps: 40 / 50\n",
            "\tloss_policy: 1.7566\n",
            "\tloss_value : 0.5457\n",
            "[2024-04-18 10:29:12] op training steps: 50 / 50\n",
            "\tloss_policy: 1.6351\n",
            "\tloss_value : 0.8081\n",
            "[2024-04-18 10:29:12] iteration 4:\n",
            "[2024-04-18 10:29:12] sp games: 10 / 10\n",
            "[2024-04-18 10:29:12] op training steps: 10 / 50\n",
            "\tloss_policy: 1.7884\n",
            "\tloss_value : 0.4645\n",
            "[2024-04-18 10:29:12] op training steps: 20 / 50\n",
            "\tloss_policy: 1.5984\n",
            "\tloss_value : 0.6009\n",
            "[2024-04-18 10:29:12] op training steps: 30 / 50\n",
            "\tloss_policy: 1.4953\n",
            "\tloss_value : 0.4519\n",
            "[2024-04-18 10:29:13] op training steps: 40 / 50\n",
            "\tloss_policy: 1.6402\n",
            "\tloss_value : 1.0160\n",
            "[2024-04-18 10:29:13] op training steps: 50 / 50\n",
            "\tloss_policy: 1.5517\n",
            "\tloss_value : 0.5907\n",
            "[2024-04-18 10:29:13] iteration 5:\n",
            "[2024-04-18 10:29:13] sp games: 10 / 10\n",
            "[2024-04-18 10:29:13] op training steps: 10 / 50\n",
            "\tloss_policy: 1.5357\n",
            "\tloss_value : 0.6804\n",
            "[2024-04-18 10:29:14] op training steps: 20 / 50\n",
            "\tloss_policy: 1.4700\n",
            "\tloss_value : 0.6742\n",
            "[2024-04-18 10:29:14] op training steps: 30 / 50\n",
            "\tloss_policy: 1.7134\n",
            "\tloss_value : 0.7123\n",
            "[2024-04-18 10:29:14] op training steps: 40 / 50\n",
            "\tloss_policy: 1.6833\n",
            "\tloss_value : 0.6129\n",
            "[2024-04-18 10:29:14] op training steps: 50 / 50\n",
            "\tloss_policy: 1.5360\n",
            "\tloss_value : 0.6889\n",
            "[2024-04-18 10:29:14] evaluation:\n",
            "\titeration 5 vs. 0: {'winP1': 11, 'winP2': 38, 'draw': 2, 'lossP1': 39, 'lossP2': 10}\n",
            "\titeration 5 vs. 0 win rate: 50.00%\n",
            "[2024-04-18 10:29:18] iteration 6:\n",
            "[2024-04-18 10:29:19] sp games: 10 / 10\n",
            "[2024-04-18 10:29:19] op training steps: 10 / 50\n",
            "\tloss_policy: 1.4698\n",
            "\tloss_value : 0.5626\n",
            "[2024-04-18 10:29:19] op training steps: 20 / 50\n",
            "\tloss_policy: 1.3500\n",
            "\tloss_value : 0.7205\n",
            "[2024-04-18 10:29:20] op training steps: 30 / 50\n",
            "\tloss_policy: 1.2531\n",
            "\tloss_value : 0.8356\n",
            "[2024-04-18 10:29:20] op training steps: 40 / 50\n",
            "\tloss_policy: 1.4493\n",
            "\tloss_value : 0.9467\n",
            "[2024-04-18 10:29:20] op training steps: 50 / 50\n",
            "\tloss_policy: 1.3255\n",
            "\tloss_value : 0.7724\n",
            "[2024-04-18 10:29:20] iteration 7:\n",
            "[2024-04-18 10:29:20] sp games: 10 / 10\n",
            "[2024-04-18 10:29:21] op training steps: 10 / 50\n",
            "\tloss_policy: 1.2194\n",
            "\tloss_value : 0.6271\n",
            "[2024-04-18 10:29:21] op training steps: 20 / 50\n",
            "\tloss_policy: 1.3288\n",
            "\tloss_value : 0.4083\n",
            "[2024-04-18 10:29:21] op training steps: 30 / 50\n",
            "\tloss_policy: 1.1441\n",
            "\tloss_value : 0.6882\n",
            "[2024-04-18 10:29:21] op training steps: 40 / 50\n",
            "\tloss_policy: 1.3455\n",
            "\tloss_value : 0.6201\n",
            "[2024-04-18 10:29:21] op training steps: 50 / 50\n",
            "\tloss_policy: 1.1971\n",
            "\tloss_value : 0.5860\n",
            "[2024-04-18 10:29:21] iteration 8:\n",
            "[2024-04-18 10:29:22] sp games: 10 / 10\n",
            "[2024-04-18 10:29:22] op training steps: 10 / 50\n",
            "\tloss_policy: 1.1213\n",
            "\tloss_value : 0.7767\n",
            "[2024-04-18 10:29:22] op training steps: 20 / 50\n",
            "\tloss_policy: 1.1010\n",
            "\tloss_value : 0.7637\n",
            "[2024-04-18 10:29:22] op training steps: 30 / 50\n",
            "\tloss_policy: 1.3249\n",
            "\tloss_value : 0.8900\n",
            "[2024-04-18 10:29:22] op training steps: 40 / 50\n",
            "\tloss_policy: 1.2639\n",
            "\tloss_value : 0.7920\n",
            "[2024-04-18 10:29:22] op training steps: 50 / 50\n",
            "\tloss_policy: 1.0536\n",
            "\tloss_value : 0.5236\n",
            "[2024-04-18 10:29:22] iteration 9:\n",
            "[2024-04-18 10:29:23] sp games: 10 / 10\n",
            "[2024-04-18 10:29:23] op training steps: 10 / 50\n",
            "\tloss_policy: 1.1319\n",
            "\tloss_value : 0.8143\n",
            "[2024-04-18 10:29:23] op training steps: 20 / 50\n",
            "\tloss_policy: 1.1669\n",
            "\tloss_value : 0.8196\n",
            "[2024-04-18 10:29:23] op training steps: 30 / 50\n",
            "\tloss_policy: 1.1022\n",
            "\tloss_value : 0.8701\n",
            "[2024-04-18 10:29:23] op training steps: 40 / 50\n",
            "\tloss_policy: 0.9545\n",
            "\tloss_value : 1.0390\n",
            "[2024-04-18 10:29:23] op training steps: 50 / 50\n",
            "\tloss_policy: 0.9255\n",
            "\tloss_value : 0.8975\n",
            "[2024-04-18 10:29:23] iteration 10:\n",
            "[2024-04-18 10:29:24] sp games: 10 / 10\n",
            "[2024-04-18 10:29:24] op training steps: 10 / 50\n",
            "\tloss_policy: 1.2434\n",
            "\tloss_value : 1.1595\n",
            "[2024-04-18 10:29:24] op training steps: 20 / 50\n",
            "\tloss_policy: 1.1899\n",
            "\tloss_value : 0.8320\n",
            "[2024-04-18 10:29:24] op training steps: 30 / 50\n",
            "\tloss_policy: 0.9519\n",
            "\tloss_value : 0.5107\n",
            "[2024-04-18 10:29:24] op training steps: 40 / 50\n",
            "\tloss_policy: 0.9734\n",
            "\tloss_value : 0.6163\n",
            "[2024-04-18 10:29:24] op training steps: 50 / 50\n",
            "\tloss_policy: 1.0136\n",
            "\tloss_value : 0.4110\n",
            "[2024-04-18 10:29:24] evaluation:\n",
            "\titeration 10 vs. 0: {'winP1': 50, 'winP2': 50, 'draw': 0, 'lossP1': 0, 'lossP2': 0}\n",
            "\titeration 10 vs. 0 win rate: 100.00%\n",
            "\titeration 10 vs. 5: {'winP1': 39, 'winP2': 0, 'draw': 11, 'lossP1': 0, 'lossP2': 50}\n",
            "\titeration 10 vs. 5 win rate: 44.50%\n",
            "[2024-04-18 10:29:32] iteration 11:\n",
            "[2024-04-18 10:29:33] sp games: 10 / 10\n",
            "[2024-04-18 10:29:33] op training steps: 10 / 50\n",
            "\tloss_policy: 0.6030\n",
            "\tloss_value : 0.9050\n",
            "[2024-04-18 10:29:33] op training steps: 20 / 50\n",
            "\tloss_policy: 0.8567\n",
            "\tloss_value : 1.2396\n",
            "[2024-04-18 10:29:33] op training steps: 30 / 50\n",
            "\tloss_policy: 0.6188\n",
            "\tloss_value : 0.9734\n",
            "[2024-04-18 10:29:33] op training steps: 40 / 50\n",
            "\tloss_policy: 0.5656\n",
            "\tloss_value : 1.3131\n",
            "[2024-04-18 10:29:34] op training steps: 50 / 50\n",
            "\tloss_policy: 0.5521\n",
            "\tloss_value : 1.0930\n",
            "[2024-04-18 10:29:34] iteration 12:\n",
            "[2024-04-18 10:29:34] sp games: 10 / 10\n",
            "[2024-04-18 10:29:34] op training steps: 10 / 50\n",
            "\tloss_policy: 0.7829\n",
            "\tloss_value : 1.1328\n",
            "[2024-04-18 10:29:34] op training steps: 20 / 50\n",
            "\tloss_policy: 0.5308\n",
            "\tloss_value : 0.5791\n",
            "[2024-04-18 10:29:34] op training steps: 30 / 50\n",
            "\tloss_policy: 0.5943\n",
            "\tloss_value : 1.3657\n",
            "[2024-04-18 10:29:34] op training steps: 40 / 50\n",
            "\tloss_policy: 0.8380\n",
            "\tloss_value : 1.0435\n",
            "[2024-04-18 10:29:34] op training steps: 50 / 50\n",
            "\tloss_policy: 0.6059\n",
            "\tloss_value : 1.0676\n",
            "[2024-04-18 10:29:34] iteration 13:\n",
            "[2024-04-18 10:29:35] sp games: 10 / 10\n",
            "[2024-04-18 10:29:35] op training steps: 10 / 50\n",
            "\tloss_policy: 0.5370\n",
            "\tloss_value : 0.9770\n",
            "[2024-04-18 10:29:35] op training steps: 20 / 50\n",
            "\tloss_policy: 0.3994\n",
            "\tloss_value : 1.1092\n",
            "[2024-04-18 10:29:35] op training steps: 30 / 50\n",
            "\tloss_policy: 0.3811\n",
            "\tloss_value : 0.8422\n",
            "[2024-04-18 10:29:35] op training steps: 40 / 50\n",
            "\tloss_policy: 0.4879\n",
            "\tloss_value : 0.6861\n",
            "[2024-04-18 10:29:36] op training steps: 50 / 50\n",
            "\tloss_policy: 0.5132\n",
            "\tloss_value : 0.8373\n",
            "[2024-04-18 10:29:36] iteration 14:\n",
            "[2024-04-18 10:29:36] sp games: 10 / 10\n",
            "[2024-04-18 10:29:36] op training steps: 10 / 50\n",
            "\tloss_policy: 0.3946\n",
            "\tloss_value : 0.6618\n",
            "[2024-04-18 10:29:36] op training steps: 20 / 50\n",
            "\tloss_policy: 0.2644\n",
            "\tloss_value : 1.0836\n",
            "[2024-04-18 10:29:36] op training steps: 30 / 50\n",
            "\tloss_policy: 0.3358\n",
            "\tloss_value : 0.8275\n",
            "[2024-04-18 10:29:36] op training steps: 40 / 50\n",
            "\tloss_policy: 0.3341\n",
            "\tloss_value : 0.6722\n",
            "[2024-04-18 10:29:37] op training steps: 50 / 50\n",
            "\tloss_policy: 0.3479\n",
            "\tloss_value : 0.7819\n",
            "[2024-04-18 10:29:37] iteration 15:\n",
            "[2024-04-18 10:29:37] sp games: 10 / 10\n",
            "[2024-04-18 10:29:37] op training steps: 10 / 50\n",
            "\tloss_policy: 0.3707\n",
            "\tloss_value : 1.0147\n",
            "[2024-04-18 10:29:37] op training steps: 20 / 50\n",
            "\tloss_policy: 0.3069\n",
            "\tloss_value : 0.6581\n",
            "[2024-04-18 10:29:37] op training steps: 30 / 50\n",
            "\tloss_policy: 0.3550\n",
            "\tloss_value : 0.9716\n",
            "[2024-04-18 10:29:37] op training steps: 40 / 50\n",
            "\tloss_policy: 0.2598\n",
            "\tloss_value : 1.1229\n",
            "[2024-04-18 10:29:38] op training steps: 50 / 50\n",
            "\tloss_policy: 0.2804\n",
            "\tloss_value : 0.8621\n",
            "[2024-04-18 10:29:38] evaluation:\n",
            "\titeration 15 vs. 0: {'winP1': 50, 'winP2': 50, 'draw': 0, 'lossP1': 0, 'lossP2': 0}\n",
            "\titeration 15 vs. 0 win rate: 100.00%\n",
            "\titeration 15 vs. 10: {'winP1': 50, 'winP2': 50, 'draw': 0, 'lossP1': 0, 'lossP2': 0}\n",
            "\titeration 15 vs. 10 win rate: 100.00%\n",
            "[2024-04-18 10:29:46] iteration 16:\n",
            "[2024-04-18 10:29:47] sp games: 10 / 10\n",
            "[2024-04-18 10:29:47] op training steps: 10 / 50\n",
            "\tloss_policy: 0.4153\n",
            "\tloss_value : 0.3753\n",
            "[2024-04-18 10:29:47] op training steps: 20 / 50\n",
            "\tloss_policy: 0.2052\n",
            "\tloss_value : 0.9488\n",
            "[2024-04-18 10:29:47] op training steps: 30 / 50\n",
            "\tloss_policy: 0.3817\n",
            "\tloss_value : 0.3237\n",
            "[2024-04-18 10:29:47] op training steps: 40 / 50\n",
            "\tloss_policy: 0.3260\n",
            "\tloss_value : 0.5880\n",
            "[2024-04-18 10:29:47] op training steps: 50 / 50\n",
            "\tloss_policy: 0.3044\n",
            "\tloss_value : 0.4511\n",
            "[2024-04-18 10:29:48] iteration 17:\n",
            "[2024-04-18 10:29:48] sp games: 10 / 10\n",
            "[2024-04-18 10:29:48] op training steps: 10 / 50\n",
            "\tloss_policy: 0.1711\n",
            "\tloss_value : 0.1782\n",
            "[2024-04-18 10:29:48] op training steps: 20 / 50\n",
            "\tloss_policy: 0.3057\n",
            "\tloss_value : 0.1997\n",
            "[2024-04-18 10:29:48] op training steps: 30 / 50\n",
            "\tloss_policy: 0.3459\n",
            "\tloss_value : 0.8418\n",
            "[2024-04-18 10:29:48] op training steps: 40 / 50\n",
            "\tloss_policy: 0.2216\n",
            "\tloss_value : 0.6180\n",
            "[2024-04-18 10:29:49] op training steps: 50 / 50\n",
            "\tloss_policy: 0.1631\n",
            "\tloss_value : 0.2859\n",
            "[2024-04-18 10:29:49] iteration 18:\n",
            "[2024-04-18 10:29:49] sp games: 10 / 10\n",
            "[2024-04-18 10:29:49] op training steps: 10 / 50\n",
            "\tloss_policy: 0.4362\n",
            "\tloss_value : 0.3717\n",
            "[2024-04-18 10:29:49] op training steps: 20 / 50\n",
            "\tloss_policy: 0.3278\n",
            "\tloss_value : 0.5198\n",
            "[2024-04-18 10:29:49] op training steps: 30 / 50\n",
            "\tloss_policy: 0.3287\n",
            "\tloss_value : 0.2308\n",
            "[2024-04-18 10:29:50] op training steps: 40 / 50\n",
            "\tloss_policy: 0.3726\n",
            "\tloss_value : 0.2297\n",
            "[2024-04-18 10:29:50] op training steps: 50 / 50\n",
            "\tloss_policy: 0.3139\n",
            "\tloss_value : 0.5977\n",
            "[2024-04-18 10:29:50] iteration 19:\n",
            "[2024-04-18 10:29:50] sp games: 10 / 10\n",
            "[2024-04-18 10:29:50] op training steps: 10 / 50\n",
            "\tloss_policy: 0.3867\n",
            "\tloss_value : 0.2736\n",
            "[2024-04-18 10:29:50] op training steps: 20 / 50\n",
            "\tloss_policy: 0.4363\n",
            "\tloss_value : 0.3736\n",
            "[2024-04-18 10:29:50] op training steps: 30 / 50\n",
            "\tloss_policy: 0.4486\n",
            "\tloss_value : 0.3298\n",
            "[2024-04-18 10:29:51] op training steps: 40 / 50\n",
            "\tloss_policy: 0.3875\n",
            "\tloss_value : 0.7810\n",
            "[2024-04-18 10:29:51] op training steps: 50 / 50\n",
            "\tloss_policy: 0.4161\n",
            "\tloss_value : 0.5956\n",
            "[2024-04-18 10:29:51] iteration 20:\n",
            "[2024-04-18 10:29:51] sp games: 10 / 10\n",
            "[2024-04-18 10:29:51] op training steps: 10 / 50\n",
            "\tloss_policy: 0.3556\n",
            "\tloss_value : 0.4141\n",
            "[2024-04-18 10:29:51] op training steps: 20 / 50\n",
            "\tloss_policy: 0.3680\n",
            "\tloss_value : 0.1648\n",
            "[2024-04-18 10:29:52] op training steps: 30 / 50\n",
            "\tloss_policy: 0.4881\n",
            "\tloss_value : 0.2439\n",
            "[2024-04-18 10:29:52] op training steps: 40 / 50\n",
            "\tloss_policy: 0.1624\n",
            "\tloss_value : 0.2964\n",
            "[2024-04-18 10:29:52] op training steps: 50 / 50\n",
            "\tloss_policy: 0.1553\n",
            "\tloss_value : 0.2727\n",
            "[2024-04-18 10:29:52] evaluation:\n",
            "\titeration 20 vs. 0: {'winP1': 50, 'winP2': 50, 'draw': 0, 'lossP1': 0, 'lossP2': 0}\n",
            "\titeration 20 vs. 0 win rate: 100.00%\n",
            "\titeration 20 vs. 15: {'winP1': 50, 'winP2': 0, 'draw': 8, 'lossP1': 0, 'lossP2': 42}\n",
            "\titeration 20 vs. 15 win rate: 54.00%\n",
            "[2024-04-18 10:30:00] iteration 21:\n",
            "[2024-04-18 10:30:01] sp games: 10 / 10\n",
            "[2024-04-18 10:30:01] op training steps: 10 / 50\n",
            "\tloss_policy: 0.6745\n",
            "\tloss_value : 0.3138\n",
            "[2024-04-18 10:30:01] op training steps: 20 / 50\n",
            "\tloss_policy: 0.4269\n",
            "\tloss_value : 0.4470\n",
            "[2024-04-18 10:30:01] op training steps: 30 / 50\n",
            "\tloss_policy: 0.2790\n",
            "\tloss_value : 0.3559\n",
            "[2024-04-18 10:30:01] op training steps: 40 / 50\n",
            "\tloss_policy: 0.3076\n",
            "\tloss_value : 0.3944\n",
            "[2024-04-18 10:30:01] op training steps: 50 / 50\n",
            "\tloss_policy: 0.5053\n",
            "\tloss_value : 0.3893\n",
            "[2024-04-18 10:30:01] iteration 22:\n",
            "[2024-04-18 10:30:02] sp games: 10 / 10\n",
            "[2024-04-18 10:30:02] op training steps: 10 / 50\n",
            "\tloss_policy: 0.3429\n",
            "\tloss_value : 0.2192\n",
            "[2024-04-18 10:30:02] op training steps: 20 / 50\n",
            "\tloss_policy: 0.2623\n",
            "\tloss_value : 0.3157\n",
            "[2024-04-18 10:30:02] op training steps: 30 / 50\n",
            "\tloss_policy: 0.2145\n",
            "\tloss_value : 0.2859\n",
            "[2024-04-18 10:30:02] op training steps: 40 / 50\n",
            "\tloss_policy: 0.3016\n",
            "\tloss_value : 0.2391\n",
            "[2024-04-18 10:30:02] op training steps: 50 / 50\n",
            "\tloss_policy: 0.4210\n",
            "\tloss_value : 0.1896\n",
            "[2024-04-18 10:30:03] iteration 23:\n",
            "[2024-04-18 10:30:03] sp games: 10 / 10\n",
            "[2024-04-18 10:30:03] op training steps: 10 / 50\n",
            "\tloss_policy: 0.2974\n",
            "\tloss_value : 0.3593\n",
            "[2024-04-18 10:30:03] op training steps: 20 / 50\n",
            "\tloss_policy: 0.2646\n",
            "\tloss_value : 0.2514\n",
            "[2024-04-18 10:30:04] op training steps: 30 / 50\n",
            "\tloss_policy: 0.4271\n",
            "\tloss_value : 0.2480\n",
            "[2024-04-18 10:30:04] op training steps: 40 / 50\n",
            "\tloss_policy: 0.3871\n",
            "\tloss_value : 0.2911\n",
            "[2024-04-18 10:30:04] op training steps: 50 / 50\n",
            "\tloss_policy: 0.4167\n",
            "\tloss_value : 0.1789\n",
            "[2024-04-18 10:30:04] iteration 24:\n",
            "[2024-04-18 10:30:04] sp games: 10 / 10\n",
            "[2024-04-18 10:30:04] op training steps: 10 / 50\n",
            "\tloss_policy: 0.4037\n",
            "\tloss_value : 0.3654\n",
            "[2024-04-18 10:30:05] op training steps: 20 / 50\n",
            "\tloss_policy: 0.4177\n",
            "\tloss_value : 0.2574\n",
            "[2024-04-18 10:30:05] op training steps: 30 / 50\n",
            "\tloss_policy: 0.2722\n",
            "\tloss_value : 0.2328\n",
            "[2024-04-18 10:30:05] op training steps: 40 / 50\n",
            "\tloss_policy: 0.1953\n",
            "\tloss_value : 0.4373\n",
            "[2024-04-18 10:30:05] op training steps: 50 / 50\n",
            "\tloss_policy: 0.3082\n",
            "\tloss_value : 0.2190\n",
            "[2024-04-18 10:30:05] iteration 25:\n",
            "[2024-04-18 10:30:06] sp games: 10 / 10\n",
            "[2024-04-18 10:30:06] op training steps: 10 / 50\n",
            "\tloss_policy: 0.1750\n",
            "\tloss_value : 0.1847\n",
            "[2024-04-18 10:30:06] op training steps: 20 / 50\n",
            "\tloss_policy: 0.1373\n",
            "\tloss_value : 0.2723\n",
            "[2024-04-18 10:30:06] op training steps: 30 / 50\n",
            "\tloss_policy: 0.3934\n",
            "\tloss_value : 0.2894\n",
            "[2024-04-18 10:30:06] op training steps: 40 / 50\n",
            "\tloss_policy: 0.2637\n",
            "\tloss_value : 0.3224\n",
            "[2024-04-18 10:30:06] op training steps: 50 / 50\n",
            "\tloss_policy: 0.2598\n",
            "\tloss_value : 0.2479\n",
            "[2024-04-18 10:30:06] evaluation:\n",
            "\titeration 25 vs. 0: {'winP1': 50, 'winP2': 50, 'draw': 0, 'lossP1': 0, 'lossP2': 0}\n",
            "\titeration 25 vs. 0 win rate: 100.00%\n",
            "\titeration 25 vs. 20: {'winP1': 41, 'winP2': 0, 'draw': 59, 'lossP1': 0, 'lossP2': 0}\n",
            "\titeration 25 vs. 20 win rate: 70.50%\n",
            "[2024-04-18 10:30:16] iteration 26:\n",
            "[2024-04-18 10:30:16] sp games: 10 / 10\n",
            "[2024-04-18 10:30:16] op training steps: 10 / 50\n",
            "\tloss_policy: 0.0665\n",
            "\tloss_value : 0.1293\n",
            "[2024-04-18 10:30:17] op training steps: 20 / 50\n",
            "\tloss_policy: 0.2629\n",
            "\tloss_value : 0.2354\n",
            "[2024-04-18 10:30:17] op training steps: 30 / 50\n",
            "\tloss_policy: 0.1392\n",
            "\tloss_value : 0.1319\n",
            "[2024-04-18 10:30:17] op training steps: 40 / 50\n",
            "\tloss_policy: 0.0508\n",
            "\tloss_value : 0.2492\n",
            "[2024-04-18 10:30:17] op training steps: 50 / 50\n",
            "\tloss_policy: 0.0740\n",
            "\tloss_value : 0.1638\n",
            "[2024-04-18 10:30:17] iteration 27:\n",
            "[2024-04-18 10:30:18] sp games: 10 / 10\n",
            "[2024-04-18 10:30:18] op training steps: 10 / 50\n",
            "\tloss_policy: 0.0778\n",
            "\tloss_value : 0.1272\n",
            "[2024-04-18 10:30:18] op training steps: 20 / 50\n",
            "\tloss_policy: 0.0981\n",
            "\tloss_value : 0.1822\n",
            "[2024-04-18 10:30:18] op training steps: 30 / 50\n",
            "\tloss_policy: 0.1112\n",
            "\tloss_value : 0.0879\n",
            "[2024-04-18 10:30:18] op training steps: 40 / 50\n",
            "\tloss_policy: 0.0300\n",
            "\tloss_value : 0.1297\n",
            "[2024-04-18 10:30:18] op training steps: 50 / 50\n",
            "\tloss_policy: 0.0644\n",
            "\tloss_value : 0.1460\n",
            "[2024-04-18 10:30:18] iteration 28:\n",
            "[2024-04-18 10:30:19] sp games: 10 / 10\n",
            "[2024-04-18 10:30:19] op training steps: 10 / 50\n",
            "\tloss_policy: 0.0737\n",
            "\tloss_value : 0.0457\n",
            "[2024-04-18 10:30:19] op training steps: 20 / 50\n",
            "\tloss_policy: 0.0376\n",
            "\tloss_value : 0.0272\n",
            "[2024-04-18 10:30:19] op training steps: 30 / 50\n",
            "\tloss_policy: 0.0276\n",
            "\tloss_value : 0.0316\n",
            "[2024-04-18 10:30:20] op training steps: 40 / 50\n",
            "\tloss_policy: 0.0058\n",
            "\tloss_value : 0.0992\n",
            "[2024-04-18 10:30:20] op training steps: 50 / 50\n",
            "\tloss_policy: 0.0267\n",
            "\tloss_value : 0.0992\n",
            "[2024-04-18 10:30:20] iteration 29:\n",
            "[2024-04-18 10:30:20] sp games: 10 / 10\n",
            "[2024-04-18 10:30:20] op training steps: 10 / 50\n",
            "\tloss_policy: 0.0093\n",
            "\tloss_value : 0.0005\n",
            "[2024-04-18 10:30:21] op training steps: 20 / 50\n",
            "\tloss_policy: 0.0239\n",
            "\tloss_value : 0.0011\n",
            "[2024-04-18 10:30:21] op training steps: 30 / 50\n",
            "\tloss_policy: 0.0241\n",
            "\tloss_value : 0.0024\n",
            "[2024-04-18 10:30:21] op training steps: 40 / 50\n",
            "\tloss_policy: 0.0460\n",
            "\tloss_value : 0.0004\n",
            "[2024-04-18 10:30:21] op training steps: 50 / 50\n",
            "\tloss_policy: 0.0040\n",
            "\tloss_value : 0.0016\n",
            "[2024-04-18 10:30:21] iteration 30:\n",
            "[2024-04-18 10:30:22] sp games: 10 / 10\n",
            "[2024-04-18 10:30:22] op training steps: 10 / 50\n",
            "\tloss_policy: 0.0045\n",
            "\tloss_value : 0.0002\n",
            "[2024-04-18 10:30:22] op training steps: 20 / 50\n",
            "\tloss_policy: 0.0031\n",
            "\tloss_value : 0.0015\n",
            "[2024-04-18 10:30:22] op training steps: 30 / 50\n",
            "\tloss_policy: 0.0266\n",
            "\tloss_value : 0.0007\n",
            "[2024-04-18 10:30:23] op training steps: 40 / 50\n",
            "\tloss_policy: 0.0031\n",
            "\tloss_value : 0.0018\n",
            "[2024-04-18 10:30:23] op training steps: 50 / 50\n",
            "\tloss_policy: 0.0026\n",
            "\tloss_value : 0.0018\n",
            "[2024-04-18 10:30:23] evaluation:\n",
            "\titeration 30 vs. 0: {'winP1': 50, 'winP2': 50, 'draw': 0, 'lossP1': 0, 'lossP2': 0}\n",
            "\titeration 30 vs. 0 win rate: 100.00%\n",
            "\titeration 30 vs. 25: {'winP1': 0, 'winP2': 0, 'draw': 100, 'lossP1': 0, 'lossP2': 0}\n",
            "\titeration 30 vs. 25 win rate: 50.00%\n",
            "[2024-04-18 10:30:32] iteration 31:\n",
            "[2024-04-18 10:30:33] sp games: 10 / 10\n",
            "[2024-04-18 10:30:33] op training steps: 10 / 50\n",
            "\tloss_policy: 0.0031\n",
            "\tloss_value : 0.0019\n",
            "[2024-04-18 10:30:33] op training steps: 20 / 50\n",
            "\tloss_policy: 0.0022\n",
            "\tloss_value : 0.0003\n",
            "[2024-04-18 10:30:33] op training steps: 30 / 50\n",
            "\tloss_policy: 0.0023\n",
            "\tloss_value : 0.0028\n",
            "[2024-04-18 10:30:33] op training steps: 40 / 50\n",
            "\tloss_policy: 0.0029\n",
            "\tloss_value : 0.0007\n",
            "[2024-04-18 10:30:34] op training steps: 50 / 50\n",
            "\tloss_policy: 0.0016\n",
            "\tloss_value : 0.0018\n",
            "[2024-04-18 10:30:34] iteration 32:\n",
            "[2024-04-18 10:30:34] sp games: 10 / 10\n",
            "[2024-04-18 10:30:34] op training steps: 10 / 50\n",
            "\tloss_policy: 0.0015\n",
            "\tloss_value : 0.0004\n",
            "[2024-04-18 10:30:35] op training steps: 20 / 50\n",
            "\tloss_policy: 0.0011\n",
            "\tloss_value : 0.0045\n",
            "[2024-04-18 10:30:35] op training steps: 30 / 50\n",
            "\tloss_policy: 0.0040\n",
            "\tloss_value : 0.0003\n",
            "[2024-04-18 10:30:35] op training steps: 40 / 50\n",
            "\tloss_policy: 0.0020\n",
            "\tloss_value : 0.0050\n",
            "[2024-04-18 10:30:35] op training steps: 50 / 50\n",
            "\tloss_policy: 0.0026\n",
            "\tloss_value : 0.0022\n",
            "[2024-04-18 10:30:35] iteration 33:\n",
            "[2024-04-18 10:30:36] sp games: 10 / 10\n",
            "[2024-04-18 10:30:36] op training steps: 10 / 50\n",
            "\tloss_policy: 0.0019\n",
            "\tloss_value : 0.0011\n",
            "[2024-04-18 10:30:37] op training steps: 20 / 50\n",
            "\tloss_policy: 0.0014\n",
            "\tloss_value : 0.0020\n",
            "[2024-04-18 10:30:37] op training steps: 30 / 50\n",
            "\tloss_policy: 0.0013\n",
            "\tloss_value : 0.0009\n",
            "[2024-04-18 10:30:37] op training steps: 40 / 50\n",
            "\tloss_policy: 0.0012\n",
            "\tloss_value : 0.0010\n",
            "[2024-04-18 10:30:37] op training steps: 50 / 50\n",
            "\tloss_policy: 0.0012\n",
            "\tloss_value : 0.0003\n",
            "[2024-04-18 10:30:37] iteration 34:\n",
            "[2024-04-18 10:30:38] sp games: 10 / 10\n",
            "[2024-04-18 10:30:38] op training steps: 10 / 50\n",
            "\tloss_policy: 0.0013\n",
            "\tloss_value : 0.0010\n",
            "[2024-04-18 10:30:38] op training steps: 20 / 50\n",
            "\tloss_policy: 0.0010\n",
            "\tloss_value : 0.0051\n",
            "[2024-04-18 10:30:38] op training steps: 30 / 50\n",
            "\tloss_policy: 0.0011\n",
            "\tloss_value : 0.0017\n",
            "[2024-04-18 10:30:38] op training steps: 40 / 50\n",
            "\tloss_policy: 0.0010\n",
            "\tloss_value : 0.0012\n",
            "[2024-04-18 10:30:39] op training steps: 50 / 50\n",
            "\tloss_policy: 0.0011\n",
            "\tloss_value : 0.0006\n",
            "[2024-04-18 10:30:39] iteration 35:\n",
            "[2024-04-18 10:30:39] sp games: 10 / 10\n",
            "[2024-04-18 10:30:39] op training steps: 10 / 50\n",
            "\tloss_policy: 0.0015\n",
            "\tloss_value : 0.0005\n",
            "[2024-04-18 10:30:39] op training steps: 20 / 50\n",
            "\tloss_policy: 0.0010\n",
            "\tloss_value : 0.0018\n",
            "[2024-04-18 10:30:40] op training steps: 30 / 50\n",
            "\tloss_policy: 0.0020\n",
            "\tloss_value : 0.0001\n",
            "[2024-04-18 10:30:40] op training steps: 40 / 50\n",
            "\tloss_policy: 0.0007\n",
            "\tloss_value : 0.0005\n",
            "[2024-04-18 10:30:40] op training steps: 50 / 50\n",
            "\tloss_policy: 0.0006\n",
            "\tloss_value : 0.0017\n",
            "[2024-04-18 10:30:40] evaluation:\n",
            "\titeration 35 vs. 0: {'winP1': 50, 'winP2': 46, 'draw': 4, 'lossP1': 0, 'lossP2': 0}\n",
            "\titeration 35 vs. 0 win rate: 98.00%\n",
            "\titeration 35 vs. 30: {'winP1': 0, 'winP2': 0, 'draw': 100, 'lossP1': 0, 'lossP2': 0}\n",
            "\titeration 35 vs. 30 win rate: 50.00%\n",
            "[2024-04-18 10:30:50] iteration 36:\n",
            "[2024-04-18 10:30:50] sp games: 10 / 10\n",
            "[2024-04-18 10:30:51] op training steps: 10 / 50\n",
            "\tloss_policy: 0.0008\n",
            "\tloss_value : 0.0004\n",
            "[2024-04-18 10:30:51] op training steps: 20 / 50\n",
            "\tloss_policy: 0.0013\n",
            "\tloss_value : 0.0003\n",
            "[2024-04-18 10:30:51] op training steps: 30 / 50\n",
            "\tloss_policy: 0.0013\n",
            "\tloss_value : 0.0003\n",
            "[2024-04-18 10:30:51] op training steps: 40 / 50\n",
            "\tloss_policy: 0.0010\n",
            "\tloss_value : 0.0008\n",
            "[2024-04-18 10:30:51] op training steps: 50 / 50\n",
            "\tloss_policy: 0.0011\n",
            "\tloss_value : 0.0007\n",
            "[2024-04-18 10:30:51] iteration 37:\n",
            "[2024-04-18 10:30:52] sp games: 10 / 10\n",
            "[2024-04-18 10:30:52] op training steps: 10 / 50\n",
            "\tloss_policy: 0.0008\n",
            "\tloss_value : 0.0005\n",
            "[2024-04-18 10:30:52] op training steps: 20 / 50\n",
            "\tloss_policy: 0.0012\n",
            "\tloss_value : 0.0011\n",
            "[2024-04-18 10:30:52] op training steps: 30 / 50\n",
            "\tloss_policy: 0.0010\n",
            "\tloss_value : 0.0018\n",
            "[2024-04-18 10:30:52] op training steps: 40 / 50\n",
            "\tloss_policy: 0.0011\n",
            "\tloss_value : 0.0001\n",
            "[2024-04-18 10:30:53] op training steps: 50 / 50\n",
            "\tloss_policy: 0.0007\n",
            "\tloss_value : 0.0004\n",
            "[2024-04-18 10:30:53] iteration 38:\n",
            "[2024-04-18 10:30:53] sp games: 10 / 10\n",
            "[2024-04-18 10:30:53] op training steps: 10 / 50\n",
            "\tloss_policy: 0.0009\n",
            "\tloss_value : 0.0002\n",
            "[2024-04-18 10:30:53] op training steps: 20 / 50\n",
            "\tloss_policy: 0.0012\n",
            "\tloss_value : 0.0002\n",
            "[2024-04-18 10:30:54] op training steps: 30 / 50\n",
            "\tloss_policy: 0.0011\n",
            "\tloss_value : 0.0018\n",
            "[2024-04-18 10:30:54] op training steps: 40 / 50\n",
            "\tloss_policy: 0.0010\n",
            "\tloss_value : 0.0004\n",
            "[2024-04-18 10:30:54] op training steps: 50 / 50\n",
            "\tloss_policy: 0.0007\n",
            "\tloss_value : 0.0002\n",
            "[2024-04-18 10:30:54] iteration 39:\n",
            "[2024-04-18 10:30:54] sp games: 10 / 10\n",
            "[2024-04-18 10:30:55] op training steps: 10 / 50\n",
            "\tloss_policy: 0.0005\n",
            "\tloss_value : 0.0003\n",
            "[2024-04-18 10:30:55] op training steps: 20 / 50\n",
            "\tloss_policy: 0.0005\n",
            "\tloss_value : 0.0003\n",
            "[2024-04-18 10:30:55] op training steps: 30 / 50\n",
            "\tloss_policy: 0.0011\n",
            "\tloss_value : 0.0001\n",
            "[2024-04-18 10:30:55] op training steps: 40 / 50\n",
            "\tloss_policy: 0.0006\n",
            "\tloss_value : 0.0004\n",
            "[2024-04-18 10:30:55] op training steps: 50 / 50\n",
            "\tloss_policy: 0.0009\n",
            "\tloss_value : 0.0009\n",
            "[2024-04-18 10:30:55] iteration 40:\n",
            "[2024-04-18 10:30:56] sp games: 10 / 10\n",
            "[2024-04-18 10:30:56] op training steps: 10 / 50\n",
            "\tloss_policy: 0.0008\n",
            "\tloss_value : 0.0008\n",
            "[2024-04-18 10:30:56] op training steps: 20 / 50\n",
            "\tloss_policy: 0.0008\n",
            "\tloss_value : 0.0001\n",
            "[2024-04-18 10:30:56] op training steps: 30 / 50\n",
            "\tloss_policy: 0.0005\n",
            "\tloss_value : 0.0002\n",
            "[2024-04-18 10:30:56] op training steps: 40 / 50\n",
            "\tloss_policy: 0.0011\n",
            "\tloss_value : 0.0004\n",
            "[2024-04-18 10:30:56] op training steps: 50 / 50\n",
            "\tloss_policy: 0.0008\n",
            "\tloss_value : 0.0027\n",
            "[2024-04-18 10:30:57] evaluation:\n",
            "\titeration 40 vs. 0: {'winP1': 50, 'winP2': 49, 'draw': 1, 'lossP1': 0, 'lossP2': 0}\n",
            "\titeration 40 vs. 0 win rate: 99.50%\n",
            "\titeration 40 vs. 35: {'winP1': 0, 'winP2': 0, 'draw': 100, 'lossP1': 0, 'lossP2': 0}\n",
            "\titeration 40 vs. 35 win rate: 50.00%\n",
            "[2024-04-18 10:31:06] iteration 41:\n",
            "[2024-04-18 10:31:07] sp games: 10 / 10\n",
            "[2024-04-18 10:31:07] op training steps: 10 / 50\n",
            "\tloss_policy: 0.0011\n",
            "\tloss_value : 0.0001\n",
            "[2024-04-18 10:31:07] op training steps: 20 / 50\n",
            "\tloss_policy: 0.0004\n",
            "\tloss_value : 0.0005\n",
            "[2024-04-18 10:31:08] op training steps: 30 / 50\n",
            "\tloss_policy: 0.0005\n",
            "\tloss_value : 0.0005\n",
            "[2024-04-18 10:31:08] op training steps: 40 / 50\n",
            "\tloss_policy: 0.0007\n",
            "\tloss_value : 0.0005\n",
            "[2024-04-18 10:31:08] op training steps: 50 / 50\n",
            "\tloss_policy: 0.0009\n",
            "\tloss_value : 0.0004\n",
            "[2024-04-18 10:31:08] iteration 42:\n",
            "[2024-04-18 10:31:08] sp games: 10 / 10\n",
            "[2024-04-18 10:31:09] op training steps: 10 / 50\n",
            "\tloss_policy: 0.0005\n",
            "\tloss_value : 0.0007\n",
            "[2024-04-18 10:31:09] op training steps: 20 / 50\n",
            "\tloss_policy: 0.0007\n",
            "\tloss_value : 0.0002\n",
            "[2024-04-18 10:31:09] op training steps: 30 / 50\n",
            "\tloss_policy: 0.0009\n",
            "\tloss_value : 0.0002\n",
            "[2024-04-18 10:31:09] op training steps: 40 / 50\n",
            "\tloss_policy: 0.0006\n",
            "\tloss_value : 0.0000\n",
            "[2024-04-18 10:31:09] op training steps: 50 / 50\n",
            "\tloss_policy: 0.0005\n",
            "\tloss_value : 0.0005\n",
            "[2024-04-18 10:31:09] iteration 43:\n",
            "[2024-04-18 10:31:10] sp games: 10 / 10\n",
            "[2024-04-18 10:31:10] op training steps: 10 / 50\n",
            "\tloss_policy: 0.0006\n",
            "\tloss_value : 0.0002\n",
            "[2024-04-18 10:31:10] op training steps: 20 / 50\n",
            "\tloss_policy: 0.0003\n",
            "\tloss_value : 0.0001\n",
            "[2024-04-18 10:31:10] op training steps: 30 / 50\n",
            "\tloss_policy: 0.0005\n",
            "\tloss_value : 0.0003\n",
            "[2024-04-18 10:31:10] op training steps: 40 / 50\n",
            "\tloss_policy: 0.0009\n",
            "\tloss_value : 0.0004\n",
            "[2024-04-18 10:31:11] op training steps: 50 / 50\n",
            "\tloss_policy: 0.0006\n",
            "\tloss_value : 0.0005\n",
            "[2024-04-18 10:31:11] iteration 44:\n",
            "[2024-04-18 10:31:11] sp games: 10 / 10\n",
            "[2024-04-18 10:31:11] op training steps: 10 / 50\n",
            "\tloss_policy: 0.0006\n",
            "\tloss_value : 0.0009\n",
            "[2024-04-18 10:31:11] op training steps: 20 / 50\n",
            "\tloss_policy: 0.0005\n",
            "\tloss_value : 0.0015\n",
            "[2024-04-18 10:31:12] op training steps: 30 / 50\n",
            "\tloss_policy: 0.0005\n",
            "\tloss_value : 0.0015\n",
            "[2024-04-18 10:31:12] op training steps: 40 / 50\n",
            "\tloss_policy: 0.0004\n",
            "\tloss_value : 0.0006\n",
            "[2024-04-18 10:31:12] op training steps: 50 / 50\n",
            "\tloss_policy: 0.0004\n",
            "\tloss_value : 0.0001\n",
            "[2024-04-18 10:31:12] iteration 45:\n",
            "[2024-04-18 10:31:13] sp games: 10 / 10\n",
            "[2024-04-18 10:31:13] op training steps: 10 / 50\n",
            "\tloss_policy: 0.0006\n",
            "\tloss_value : 0.0013\n",
            "[2024-04-18 10:31:13] op training steps: 20 / 50\n",
            "\tloss_policy: 0.0008\n",
            "\tloss_value : 0.0024\n",
            "[2024-04-18 10:31:13] op training steps: 30 / 50\n",
            "\tloss_policy: 0.0005\n",
            "\tloss_value : 0.0012\n",
            "[2024-04-18 10:31:13] op training steps: 40 / 50\n",
            "\tloss_policy: 0.0005\n",
            "\tloss_value : 0.0002\n",
            "[2024-04-18 10:31:14] op training steps: 50 / 50\n",
            "\tloss_policy: 0.0007\n",
            "\tloss_value : 0.0004\n",
            "[2024-04-18 10:31:14] evaluation:\n",
            "\titeration 45 vs. 0: {'winP1': 50, 'winP2': 50, 'draw': 0, 'lossP1': 0, 'lossP2': 0}\n",
            "\titeration 45 vs. 0 win rate: 100.00%\n",
            "\titeration 45 vs. 40: {'winP1': 0, 'winP2': 0, 'draw': 100, 'lossP1': 0, 'lossP2': 0}\n",
            "\titeration 45 vs. 40 win rate: 50.00%\n",
            "[2024-04-18 10:31:24] iteration 46:\n",
            "[2024-04-18 10:31:25] sp games: 10 / 10\n",
            "[2024-04-18 10:31:25] op training steps: 10 / 50\n",
            "\tloss_policy: 0.0009\n",
            "\tloss_value : 0.0002\n",
            "[2024-04-18 10:31:25] op training steps: 20 / 50\n",
            "\tloss_policy: 0.0006\n",
            "\tloss_value : 0.0009\n",
            "[2024-04-18 10:31:25] op training steps: 30 / 50\n",
            "\tloss_policy: 0.0005\n",
            "\tloss_value : 0.0004\n",
            "[2024-04-18 10:31:25] op training steps: 40 / 50\n",
            "\tloss_policy: 0.0008\n",
            "\tloss_value : 0.0003\n",
            "[2024-04-18 10:31:26] op training steps: 50 / 50\n",
            "\tloss_policy: 0.0006\n",
            "\tloss_value : 0.0002\n",
            "[2024-04-18 10:31:26] iteration 47:\n",
            "[2024-04-18 10:31:27] sp games: 10 / 10\n",
            "[2024-04-18 10:31:27] op training steps: 10 / 50\n",
            "\tloss_policy: 0.0004\n",
            "\tloss_value : 0.0001\n",
            "[2024-04-18 10:31:27] op training steps: 20 / 50\n",
            "\tloss_policy: 0.0004\n",
            "\tloss_value : 0.0001\n",
            "[2024-04-18 10:31:27] op training steps: 30 / 50\n",
            "\tloss_policy: 0.0005\n",
            "\tloss_value : 0.0009\n",
            "[2024-04-18 10:31:27] op training steps: 40 / 50\n",
            "\tloss_policy: 0.0006\n",
            "\tloss_value : 0.0002\n",
            "[2024-04-18 10:31:28] op training steps: 50 / 50\n",
            "\tloss_policy: 0.0006\n",
            "\tloss_value : 0.0002\n",
            "[2024-04-18 10:31:28] iteration 48:\n",
            "[2024-04-18 10:31:29] sp games: 10 / 10\n",
            "[2024-04-18 10:31:29] op training steps: 10 / 50\n",
            "\tloss_policy: 0.0006\n",
            "\tloss_value : 0.0006\n",
            "[2024-04-18 10:31:29] op training steps: 20 / 50\n",
            "\tloss_policy: 0.0004\n",
            "\tloss_value : 0.0021\n",
            "[2024-04-18 10:31:29] op training steps: 30 / 50\n",
            "\tloss_policy: 0.0005\n",
            "\tloss_value : 0.0013\n",
            "[2024-04-18 10:31:29] op training steps: 40 / 50\n",
            "\tloss_policy: 0.0003\n",
            "\tloss_value : 0.0004\n",
            "[2024-04-18 10:31:29] op training steps: 50 / 50\n",
            "\tloss_policy: 0.0004\n",
            "\tloss_value : 0.0017\n",
            "[2024-04-18 10:31:29] iteration 49:\n",
            "[2024-04-18 10:31:30] sp games: 10 / 10\n",
            "[2024-04-18 10:31:30] op training steps: 10 / 50\n",
            "\tloss_policy: 0.0005\n",
            "\tloss_value : 0.0001\n",
            "[2024-04-18 10:31:30] op training steps: 20 / 50\n",
            "\tloss_policy: 0.0004\n",
            "\tloss_value : 0.0002\n",
            "[2024-04-18 10:31:30] op training steps: 30 / 50\n",
            "\tloss_policy: 0.0005\n",
            "\tloss_value : 0.0002\n",
            "[2024-04-18 10:31:30] op training steps: 40 / 50\n",
            "\tloss_policy: 0.0005\n",
            "\tloss_value : 0.0002\n",
            "[2024-04-18 10:31:31] op training steps: 50 / 50\n",
            "\tloss_policy: 0.0004\n",
            "\tloss_value : 0.0012\n",
            "[2024-04-18 10:31:31] iteration 50:\n",
            "[2024-04-18 10:31:31] sp games: 10 / 10\n",
            "[2024-04-18 10:31:31] op training steps: 10 / 50\n",
            "\tloss_policy: 0.0004\n",
            "\tloss_value : 0.0001\n",
            "[2024-04-18 10:31:32] op training steps: 20 / 50\n",
            "\tloss_policy: 0.0006\n",
            "\tloss_value : 0.0001\n",
            "[2024-04-18 10:31:32] op training steps: 30 / 50\n",
            "\tloss_policy: 0.0007\n",
            "\tloss_value : 0.0004\n",
            "[2024-04-18 10:31:32] op training steps: 40 / 50\n",
            "\tloss_policy: 0.0003\n",
            "\tloss_value : 0.0001\n",
            "[2024-04-18 10:31:32] op training steps: 50 / 50\n",
            "\tloss_policy: 0.0007\n",
            "\tloss_value : 0.0006\n",
            "[2024-04-18 10:31:32] evaluation:\n",
            "\titeration 50 vs. 0: {'winP1': 50, 'winP2': 48, 'draw': 2, 'lossP1': 0, 'lossP2': 0}\n",
            "\titeration 50 vs. 0 win rate: 99.00%\n",
            "\titeration 50 vs. 45: {'winP1': 0, 'winP2': 0, 'draw': 100, 'lossP1': 0, 'lossP2': 0}\n",
            "\titeration 50 vs. 45 win rate: 50.00%\n"
          ]
        }
      ]
    }
  ]
}